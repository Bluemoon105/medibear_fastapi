# server.py
from fastapi import FastAPI, BackgroundTasks
from pydantic import BaseModel
from datetime import datetime
from typing import Dict, Any, Optional, List
from pymongo import MongoClient
import numpy as np
import asyncio
import os

# ===== Embedding =====
from sentence_transformers import SentenceTransformer
EMBED_MODEL_NAME = "intfloat/multilingual-e5-small"   # âœ… ê³ ì • (384ì°¨ì›)
embed_model = SentenceTransformer(EMBED_MODEL_NAME, device="cpu")

def embed(text: str) -> List[float]:
    vec = embed_model.encode(text, normalize_embeddings=True)
    return vec.tolist()

VECTOR_DIM = len(embed_model.encode("dim"))  # âœ… 384

# ===== LLM (Qwen 1.5B GGUF with llama.cpp) =====
from llama_cpp import Llama
MODEL_PATH = "../../models/exercise_models/qwen2.5-1.5b-instruct-q4_k_m.gguf"

llm = Llama(
    model_path=MODEL_PATH,
    n_ctx=2048,
    n_threads=max(1, (os.cpu_count() or 2) - 1),
    n_batch=128,
    logits_all=False,
    verbose=False,
    chat_format="chatml",   # <<< ğŸ”§ ë³€ê²½: qwen2 ë¡œ ì •í™•í•œ í…œí”Œë¦¿ ì‚¬ìš©
)

# ===== FastAPI =====
app = FastAPI(title="MediBear LLM Server (Local Mongo + RAG)")

# ===== MongoDB (ë¡œì»¬ë§Œ) =====
client = MongoClient("mongodb://localhost:27017", serverSelectionTimeoutMS=500)
db = client["ai_coach"]
chat_col = db["chat_history"]
profile_col = db["profile"]

# ===== Schemas =====
class ChatInput(BaseModel):
    user_id: str
    message: str

class ChatWithAnalysisInput(BaseModel):
    user_id: str
    message: str
    analysis: Dict[str, Any]

# ===== Utils =====
def cosine_similarity(a, b) -> float:
    a, b = np.asarray(a, dtype=np.float32), np.asarray(b, dtype=np.float32)
    na, nb = np.linalg.norm(a), np.linalg.norm(b)
    if na == 0.0 or nb == 0.0:
        return 0.0
    return float(np.dot(a, b) / (na * nb))

def safe_get_vec(doc) -> Optional[List[float]]:
    """
    DB í•˜ìœ„í˜¸í™˜: 'embedding' ë˜ëŠ” 'vector' í‚¤ ì‚¬ìš©.
    ì°¨ì›ì´ ë‹¤ë¥´ë©´ None ë°˜í™˜.
    """
    vec = doc.get("embedding") or doc.get("vector")
    if not isinstance(vec, list):
        return None
    if len(vec) != VECTOR_DIM:
        return None
    return vec

# ===== ë¶„ì„ê°’ â†’ ìì—°ì–´ ìƒíƒœ(ìˆ«ì ì§ì ‘ ë…¸ì¶œ ê¸ˆì§€) =====
def describe_joint(a: Optional[float]) -> str:
    if a is None: return "ë³´í†µ"
    if a > 170:   return "ì¶©ë¶„íˆ í´ì§"
    if a > 150:   return "ëŒ€ì²´ë¡œ ì–‘í˜¸"
    if a > 120:   return "ì¡°ê¸ˆ ë” í´ê¸°"
    return "í˜ ì „ë‹¬ ë¶€ì¡±"

def describe_back(a: Optional[float]) -> str:
    if a is None: return "ë³´í†µ"
    if a > 40:    return "í—ˆë¦¬ ê³¼ì‹ ì „ ê²½í–¥"
    if a < 15:    return "í—ˆë¦¬ ë§ë¦¼ ê²½í–¥"
    return "ì¤‘ë¦½ ìœ ì§€ ì–‘í˜¸"

# # ===== System Prompt (ë˜ë¬»ê¸° ê¸ˆì§€ + í˜•ì‹ ê°•ì œ) =====
# SYSTEM_PROMPT = (
#     "ë„ˆëŠ” í•œêµ­ì–´ í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆë‹¤. ë¶„ì„ ë°ì´í„°ê°€ ì´ë¯¸ ì œê³µë˜ë©°, ì ˆëŒ€ ë˜ë¬»ì§€ ì•ŠëŠ”ë‹¤.\n"
#     "íŒ”ê¿ˆì¹˜ ê°ë„, í—ˆë¦¬ ê°ë„, ìˆ˜ì¹˜, ìˆ«ì(Â°, %, cm ë“±) ì–¸ê¸‰ ê¸ˆì§€. "
#     "ìˆ«ìë¥¼ ìœ ì¶”í•´ì„œ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒë„ ê¸ˆì§€. "
#     "í•­ìƒ ê°ê° ì¤‘ì‹¬ í‘œí˜„ìœ¼ë¡œ ë°”ê¿” ë§í•œë‹¤.\n"
#     "ë¬¸ì²´ëŠ” ì§§ê³  ë‹¨í˜¸í•˜ì§€ë§Œ ë”°ëœ»í•˜ê²Œ. ë°˜ë³µ/ì—ì½” ê¸ˆì§€.\n\n"
#     "ì¶œë ¥ í˜•ì‹ì€:\n"
#     "â‘  ìì„¸ ëŠë‚Œ ìš”ì•½ (2ë¬¸ì¥)\n"
#     "â‘¡ ì˜í•œ ì  (1ë¬¸ì¥)\n"
#     "â‘¢ ê°œì„ í•  ì  (2~3ê°œ ë¶ˆë¦¿)\n"
#     "â‘£ ì½”ì¹­ í (3~5ê°œ, 4~8ê¸€ì ëª…ë ¹í˜•)\n"
#     "â‘¤ ë‹¤ìŒ ì„¸íŠ¸ ëª©í‘œ (1ë¬¸ì¥)\n"
# )
SYSTEM_PROMPT = (
    "ë„ˆëŠ” í•œêµ­ì–´ í¼ìŠ¤ë„ íŠ¸ë ˆì´ë„ˆì´ë‹¤. ì‚¬ìš©ìëŠ” ì´ë¯¸ ìš´ë™ ë¶„ì„ ë°ì´í„°ë¥¼ ì œê³µí–ˆê³ , "
    "ë„ˆëŠ” ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ì¦‰ì‹œ í”¼ë“œë°±ì„ ì œê³µ**í•´ì•¼ í•œë‹¤.\n\n"

    "ğŸš« ê¸ˆì§€ì‚¬í•­ (ì ˆëŒ€ ì–´ê¸°ì§€ ë§ ê²ƒ):\n"
    "- ìˆ«ì, ê°ë„, ë¹„ìœ¨, cm, %, Â° ë“± **ëª¨ë“  ìˆ˜ì¹˜ í‘œí˜„ ê¸ˆì§€**\n"
    "- ìˆ˜ì¹˜ë¥¼ ìœ ì¶”í•˜ê±°ë‚˜ ë§Œë“¤ì–´ë‚´ëŠ” ì„¤ëª… ê¸ˆì§€\n"
    "- 'ìˆ˜ì¹˜', 'ê°ë„', 'ë°ì´í„°', 'ì •í™•' ê°™ì€ í‘œí˜„ ê¸ˆì§€\n"
    "- ë¶„ì„ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ë°˜ë³µí•˜ê±°ë‚˜ ì„¤ëª… í˜• ë¬¸ì¥ ê¸ˆì§€\n"
    "- 'í•´ë³´ì„¸ìš”?', 'ì–´ë–¨ê¹Œìš”?' ê°™ì€ ì§ˆë¬¸í˜• ë§íˆ¬ ê¸ˆì§€\n\n"

    "âœ… í‘œí˜„ ë°©ì‹ (ì´ ìŠ¤íƒ€ì¼ì„ ê°•í•˜ê²Œ ìœ ì§€):\n"
    "- **ì½”ì¹˜ê°€ ë°”ë¡œ ì˜†ì—ì„œ ë§í•˜ë“¯** ë¶€ë“œëŸ½ê³  ë‹¨í˜¸í•˜ê²Œ\n"
    "- ê°ê° ê¸°ë°˜ í‘œí˜„ ì‚¬ìš© (ì˜ˆ: 'ê°€ìŠ´ì„ ë¶€ë“œëŸ½ê²Œ', 'íŒ”ì„ ê¸¸ê²Œ ë»—ì–´', 'ëª¸ì˜ ì¤‘ì‹¬ì„ ì‚´ì§ ëª¨ì•„')\n"
    "- ì§§ê³  ëª…í™•í•œ ë¬¸ì¥\n"
    "- *ë”°ëœ»í•˜ì§€ë§Œ í™•ì‹  ìˆëŠ” í†¤*\n\n"

    "ì¶œë ¥ í˜•ì‹ì€ ì•„ë˜ë¥¼ **ê·¸ëŒ€ë¡œ** ì‚¬ìš©í•˜ë¼:\n"
    "â‘  ìì„¸ ëŠë‚Œ ìš”ì•½ (ìì—°ìŠ¤ëŸ½ê²Œ 2ë¬¸ì¥)\n"
    "â‘¡ ì˜í•œ ì  (1ë¬¸ì¥)\n"
    "â‘¢ ê°œì„ í•  ì  (â€¢ ë¶ˆë¦¿ 2~3ê°œ)\n"
    "â‘£ ì½”ì¹­ í (â€¢ ë¶ˆë¦¿ 3~5ê°œ, 4~8ê¸€ì ëª…ë ¹í˜•)\n"
    "â‘¤ ë‹¤ìŒ ì„¸íŠ¸ ëª©í‘œ (1ë¬¸ì¥ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ)\n"
)



def build_rag_context(user_id: str, user_msg: str, topk: int = 3) -> str:
    """
    ë¡œì»¬ Mongoì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ RAG.
    (Atlas ì „ìš© ìŠ¤í…Œì´ì§€ ì‚¬ìš© ì•ˆ í•¨)
    """
    qvec = embed(user_msg)
    # ìµœê·¼ 50ê°œë§Œ ìŠ¤ìº” (ì†ë„/ë©”ëª¨ë¦¬ ê· í˜•)
    history = list(chat_col.find({"user_id": user_id})
                   .sort("timestamp", -1)
                   .limit(50))
    scored = []
    for h in history:
        vec = safe_get_vec(h)
        if vec is None:
            continue
        sim = cosine_similarity(qvec, vec)
        scored.append((sim, h.get("message", ""), h.get("response", "")))
    if not scored:
        return ""

    scored.sort(key=lambda x: x[0], reverse=True)
    picked = scored[:topk]
    ctx = []
    for _, um, ar in picked:
        ctx.append(f"User: {um}\nAI: {ar}")
    return "\n---\n".join(ctx)

def build_user_prompt(user_msg: str, analysis: Dict[str, Any], user_id: str) -> str:
    # RAG ì»¨í…ìŠ¤íŠ¸(ìˆìœ¼ë©´ ìƒë‹¨ ë°°ì¹˜í•˜ì—¬ ìš°ì„  ë°˜ì˜)
    rag = build_rag_context(user_id, user_msg)

    ex = (analysis or {}).get("detected_exercise") or "ë¯¸í™•ì¸ ìš´ë™"
    stage = (analysis or {}).get("stage") or "ë‹¨ê³„ ì •ë³´ ì—†ìŒ"
    joints = ((analysis or {}).get("pose_data") or {}).get("joints", {})

    left_elbow  = describe_joint(joints.get("left_elbow_angle"))
    right_elbow = describe_joint(joints.get("right_elbow_angle"))
    left_knee   = describe_joint(joints.get("left_knee_angle"))
    right_knee  = describe_joint(joints.get("right_knee_angle"))
    back_state  = describe_back(joints.get("back_angle"))

    lines = []
    if rag:
        lines.append("[ê³¼ê±° ìœ ì‚¬ ëŒ€í™”]\n" + rag)

    lines.append(
        "[ì‚¬ìš©ì ìš”ì²­]\n"
        f"{user_msg}\n\n"
        "[ë¶„ì„ ìƒíƒœ(ì°¸ê³ ìš©) â€” ì¶œë ¥ì— ìƒíƒœ ë‹¨ì–´ë¥¼ ê·¸ëŒ€ë¡œ ì“°ì§€ ë§ê³  ì½”ì¹­ ë¬¸ì¥ìœ¼ë¡œ í’€ì–´ì“¸ ê²ƒ]\n"
        f"- ìš´ë™: {ex}\n"
        f"- ë‹¨ê³„: {stage}\n"
        f"- íŒ”: ì¢Œ {left_elbow} / ìš° {right_elbow}\n"
        f"- ë¬´ë¦: ì¢Œ {left_knee} / ìš° {right_knee}\n"
        f"- í—ˆë¦¬: {back_state}\n\n"
        "ìœ„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í˜•ì‹ì„ ì •í™•íˆ ì§€ì¼œ **ì½”ì¹­ ë¬¸ì¥**ìœ¼ë¡œë§Œ ë‹µë³€í•´ë¼. "
        "ì§ˆë¬¸í•˜ì§€ ë§ê³  ë°”ë¡œ í”¼ë“œë°±ì„ ì œê³µí•˜ë¼."
    )
    return "\n\n".join(lines)

async def llm_generate(messages: List[Dict[str, str]]) -> str:
    def _run():
        out = llm.create_chat_completion(
            messages=messages,
            temperature=0.55,
            top_p=0.9,
            repeat_penalty=1.12,     # âœ… ì—ì½” ë°©ì§€ ë§¤ìš° ì¤‘ìš”
            max_tokens=600,          # âœ… ëŠê¹€ ë°©ì§€
            stop=["<|im_end|>"],    
        )
        return out["choices"][0]["message"]["content"].strip()
    return await asyncio.to_thread(_run)

# ===== Persona ìš”ì•½ (ë°±ê·¸ë¼ìš´ë“œ) =====
async def update_persona_background(user_id: str):
    chats = list(chat_col.find({"user_id": user_id}).sort("timestamp", -1).limit(12))
    if not chats:
        return
    text_block = "\n".join([f"User: {c.get('message','')}\nAI: {c.get('response','')}" for c in chats])

    messages = [
        {"role": "system", "content": (
            "ì•„ë˜ ìµœê·¼ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìš´ë™ ìŠµê´€/ëª©í‘œ/í†µì¦/ì„ í˜¸ë¥¼ 5ì¤„ë¡œ ìš”ì•½í•˜ë¼. "
            "ìˆ«ì ê°ë„ ë“± ì„¸ë¶€ ìˆ˜ì¹˜ëŠ” ì“°ì§€ ë§ˆë¼. ì¤‘ë³µ ì—†ì´ ê°„ê²°í•˜ê²Œ."
        )},
        {"role": "user", "content": text_block},
    ]
    summary = await llm_generate(messages)
    profile_col.update_one(
        {"user_id": user_id},
        {"$set": {"persona": summary, "updated_at": datetime.now()}},
        upsert=True
    )

# ===== ê³µí†µ ìƒì„± ë¡œì§ =====
async def generate_answer(user_id: str, user_msg: str, analysis: Dict[str, Any]) -> str:
    # Persona
    persona_doc = profile_col.find_one({"user_id": user_id})
    persona = persona_doc.get("persona") if persona_doc else ""

    user_block = build_user_prompt(user_msg, analysis, user_id)

    messages = [
        {"role": "system", "content": SYSTEM_PROMPT + ("\n[ì‚¬ìš©ì ìš”ì•½]\n" + persona if persona else "")},
        {"role": "user", "content": user_block},
    ]
    return await llm_generate(messages)

def save_chat(user_id: str, message: str, response: str, embedding: List[float], analysis: Optional[Dict[str, Any]] = None):
    chat_col.insert_one({
        "user_id": user_id,
        "message": message,
        "response": response,
        "embedding": embedding,     # âœ… í•­ìƒ 384ì°¨ì›ìœ¼ë¡œ ì €ì¥
        "analysis": analysis or {},
        "timestamp": datetime.now(),
        "embed_model": EMBED_MODEL_NAME,  # âœ… ì¶”í›„ ë§ˆì´ê·¸ë ˆì´ì…˜ ëŒ€ë¹„
        "embed_dim": VECTOR_DIM,
    })

# ===== Endpoints =====
@app.post("/chat")
async def chat_plain(data: ChatInput, background_tasks: BackgroundTasks):
    # í˜„ì¬ ë©”ì‹œì§€ ì„ë² ë”©
    qvec = embed(data.message)
    # ë‹µë³€ ìƒì„± (ë¶„ì„ ì—†ìŒ)
    answer = await generate_answer(data.user_id, data.message, analysis={})
    # ì €ì¥
    save_chat(data.user_id, data.message, answer, qvec, analysis={})
    # í˜ë¥´ì†Œë‚˜ëŠ” 3ê°œ ì´ìƒì¼ ë•Œ ì£¼ê¸°ì ìœ¼ë¡œ ê°±ì‹ 
    if chat_col.count_documents({"user_id": data.user_id}) >= 3:
        background_tasks.add_task(update_persona_background, data.user_id)
    return {"answer": answer}

@app.post("/chat_with_analysis")
async def chat_with_analysis(data: ChatWithAnalysisInput, background_tasks: BackgroundTasks):
    
    qvec = embed(data.message)
    answer = await generate_answer(data.user_id, data.message, analysis=data.analysis)
    save_chat(data.user_id, data.message, answer, qvec, analysis=data.analysis)
    if chat_col.count_documents({"user_id": data.user_id}) >= 3:
        background_tasks.add_task(update_persona_background, data.user_id)
    return {"answer": answer}



# from fastapi import FastAPI, BackgroundTasks
# from pydantic import BaseModel
# from datetime import datetime
# from pymongo import MongoClient
# from sentence_transformers import SentenceTransformer
# import numpy as np
# import asyncio
# import os
# from llama_cpp import Llama

# app = FastAPI()

# # ---------------- MongoDB ----------------
# client = MongoClient("mongodb://localhost:27017")
# db = client["ai_coach"]
# chat_col = db["chat_history"]
# profile_col = db["profile"]

# # ---------------- Embedding Model ----------------
# embed_model = SentenceTransformer("intfloat/multilingual-e5-small", device="cpu")

# # ---------------- LLM ----------------
# MODEL_PATH = "../../models/exercise_models/qwen2.5-1.5b-instruct-q4_k_m.gguf"
# llm = Llama(
#     model_path=MODEL_PATH,
#     n_ctx=1024,
#     n_threads=8,
#     n_batch=128,
#     logits_all=False,
#     verbose=False,
#     chat_format="chatml"
# )

# class ChatInput(BaseModel):
#     user_id: str
#     message: str

# def cosine_similarity(a, b):
#     a, b = np.array(a), np.array(b)
#     if np.linalg.norm(a) == 0 or np.linalg.norm(b) == 0:
#         return 0.0
#     return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b)))


# async def generate_async(user_msg: str, persona: str, context_text: str):
#     messages = [
#         {"role": "system",
#          "content": (
#              "ë‹¹ì‹ ì€ ê°œì¸ ë§ì¶¤í˜• ê±´ê°•/ìš´ë™ ìƒë‹´ ì½”ì¹˜ AIì…ë‹ˆë‹¤.\n"
#              "ì‚¬ìš©ìì˜ ì§€ë‚œ ëŒ€í™” ë‚´ìš©(persona ìš”ì•½ + ìµœê·¼ ëŒ€í™” context)ì„ ì°¸ê³ í•˜ì—¬ "
#              "ì‚¬ìš©ìì˜ ìƒíƒœì™€ ê°ì •, ìŠµê´€ì„ ê¸°ì–µí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì§€ëŠ” ëŒ€í™”ë¥¼ í•˜ì„¸ìš”."
#          )},
#         {"role": "user",
#          "content": f"[ì‚¬ìš©ì ìš”ì•½ ì •ë³´]\n{persona}\n\n[ìµœê·¼ ê´€ë ¨ ëŒ€í™”]\n{context_text}\n\n[í˜„ì¬ ì§ˆë¬¸]\n{user_msg}"}
#     ]

#     def _run():
#         out = llm.create_chat_completion(
#             messages=messages,
#             temperature=0.35,
#             top_p=0.9,
#             max_tokens=240,
#             stop=["</s>", "<|im_end|>"]
#         )
#         return out["choices"][0]["message"]["content"].strip()

#     return await asyncio.to_thread(_run)


# async def update_persona_background(user_id: str):
#     chats = list(chat_col.find({"user_id": user_id}).sort("timestamp", -1).limit(10))
#     text_block = "\n".join([f"User: {c['message']}\nAI: {c['response']}" for c in chats])

#     messages = [
#         {"role": "system", "content": "ìµœê·¼ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìì˜ ê±´ê°•/ìš´ë™ íŠ¹ì§•ì„ 5ì¤„ë¡œ ìš”ì•½í•˜ì„¸ìš”."},
#         {"role": "user", "content": text_block or "(ëŒ€í™”ì—†ìŒ)"}
#     ]

#     def _run():
#         out = llm.create_chat_completion(messages=messages, temperature=0.2, top_p=0.9, max_tokens=120)
#         return out["choices"][0]["message"]["content"].strip()

#     summary = await asyncio.to_thread(_run)

#     profile_col.update_one(
#         {"user_id": user_id},
#         {"$set": {"persona": summary, "updated_at": datetime.now()}},
#         upsert=True
#     )


# @app.post("/chat")
# async def chat_with_ai(data: ChatInput, background_tasks: BackgroundTasks):

#     # 1) ì…ë ¥ ë¬¸ì¥ ì„ë² ë”©
#     emb = embed_model.encode(data.message, normalize_embeddings=True)
#     user_vec = emb.tolist()

#     # 2) ìµœê·¼ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸° + RAG (ìœ ì‚¬ë„ ìƒìœ„ 3ê°œ)
#     history = list(chat_col.find({"user_id": data.user_id}).sort("timestamp", -1).limit(10))

#     contexts = []
#     for h in history:
#         vec = h.get("embedding") or h.get("vector")
#         if not vec:
#             continue
#         if len(vec) != len(user_vec):
#             continue    # âœ… ì°¨ì› ë‹¤ë¥´ë©´ skip
#         sim = cosine_similarity(user_vec, vec)
#         contexts.append((sim, h["message"], h.get("response", "")))

#     if contexts:
#         contexts = sorted(contexts, key=lambda x: x[0], reverse=True)[:3]
#         context_text = "\n".join([f"User: {m}\nAI: {r}" for _, m, r in contexts])
#     else:
#         context_text = "\n".join([f"User: {h['message']}\nAI: {h['response']}" for h in history[:3]])

#     # 3) Persona ë¶ˆëŸ¬ì˜¤ê¸°
#     profile = profile_col.find_one({"user_id": data.user_id})
#     persona = profile["persona"] if profile else "íŠ¹ì§• ë¯¸íŒŒì•… ì‚¬ìš©ì"

#     # 4) LLM í˜¸ì¶œ
#     answer = await generate_async(data.message, persona, context_text)

#     # 5) ì €ì¥
#     chat_col.insert_one({
#         "user_id": data.user_id,
#         "message": data.message,
#         "response": answer,
#         "embedding": user_vec,
#         "timestamp": datetime.now()
#     })

#     # 6) Persona ì—…ë°ì´íŠ¸ëŠ” ë°±ê·¸ë¼ìš´ë“œë¡œ
#     if len(history) >= 3:
#         background_tasks.add_task(update_persona_background, data.user_id)

#     return {"answer": answer, "persona_summary": persona}

